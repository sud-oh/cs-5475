{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as k\n",
    "from keras import layers as l\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up modified UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(layer_in, n_filters, batchnorm=True):\n",
    "    # Initialize weights\n",
    "    init = k.initializers.RandomNormal(0., 0.02)    # Add the downsampling\n",
    "    d = l.Conv2D(n_filters, (4,4,), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    if(batchnorm):\n",
    "        d = l.BatchNormalization()(d, training=True)\n",
    "    d = l.LeakyReLU(alpha=0.2)(d)\n",
    "    return d\n",
    "\n",
    "def decoder(layer_in, skip_in, n_filters, dropout=True):\n",
    "    # Initialize weights\n",
    "    init = k.initializers.RandomNormal(0., 0.02)    # Add the upsampling\n",
    "    u = l.Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    u = l.BatchNormalization()(u, training=True)\n",
    "    if(dropout):\n",
    "        u = l.Dropout(0.5)(u, training=True)\n",
    "    # Merge\n",
    "    u = l.Concatenate()([u, skip_in])\n",
    "    u = l.Activation('relu')(u)\n",
    "    return u\n",
    "\n",
    "def Generator(image_shape=(256,256,3)):\n",
    "    # Initialize weights\n",
    "    init = k.initializers.RandomNormal(0., 0.02)\n",
    "    # Input\n",
    "    in_image = l.Input(shape=image_shape)\n",
    "\n",
    "    # Encode\n",
    "    e1 = encoder(in_image, 64, False)\n",
    "    e2 = encoder(e1, 128)\n",
    "    e3 = encoder(e2, 256)\n",
    "    e4 = encoder(e3, 512)\n",
    "    e5 = encoder(e4, 512)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = l.Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e5)\n",
    "    b = l.Activation('relu')(b)\n",
    "\n",
    "    # Decode\n",
    "    d1 = decoder(b, e5, 512)\n",
    "    d2 = decoder(d1, e4, 512)\n",
    "    d3 = decoder(d2, e3, 256, dropout=False)\n",
    "    d4 = decoder(d3, e2, 128, dropout=False)\n",
    "    d5 = decoder(d4, e1, 64, dropout=False)\n",
    "\n",
    "    # Output\n",
    "    g = l.Conv2DTranspose(3, (4,4), strides=(2,2), padding = 'same', kernel_initializer=init)(d5)\n",
    "    out_image = l.Activation('tanh')(g)\n",
    "\n",
    "    # Define model\n",
    "    model = k.Model(inputs=in_image, outputs = out_image)\n",
    "    return model\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "LAMBDA = 100\n",
    "\n",
    "def generator_loss(disc_fake_output, generator_output, target_output):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_fake_output), disc_fake_output)\n",
    "\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target_output - generator_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup descriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(image_shape):\n",
    "    init = k.initializers.RandomNormal(0., 0.02)\n",
    "    input_image = l.Input(shape=image_shape)\n",
    "    target_image = l.Input(shape=image_shape)\n",
    "    d = l.Concatenate()([input_image, target_image])\n",
    "    \n",
    "    d = l.SpectralNormalization(l.Conv2D(64, (4, 4), strides=2, padding='same', kernel_initializer=init))(d)\n",
    "    d = l.LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = l.SpectralNormalization(l.Conv2D(128, (4, 4), strides=2, padding='same', kernel_initializer=init))(d)\n",
    "    d = l.LayerNormalization()(d, training=True)\n",
    "    d = l.LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = l.SpectralNormalization(l.Conv2D(256, (4, 4), strides=2, padding='same', kernel_initializer=init))(d)\n",
    "    d = l.LayerNormalization()(d, training=True)\n",
    "    d = l.LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    zero_pad1 = l.ZeroPadding2D()(d)\n",
    "    d = l.Conv2D(512, 4, strides=1, kernel_initializer = init,\n",
    "                 use_bias= False)(zero_pad1)\n",
    "    \n",
    "    batchnorm1 = l.BatchNormalization()(d)\n",
    "    d = l.LeakyReLU(alpha=0.2)(batchnorm1)\n",
    "\n",
    "    zero_pad2 = l.ZeroPadding2D()(d)\n",
    "    \n",
    "    d = l.SpectralNormalization(l.Conv2D(1, 4, strides=1, kernel_initializer=init))(zero_pad2)\n",
    "    \n",
    "    model = k.Model([input_image, target_image], d)\n",
    "    return model\n",
    "\n",
    "disc_ce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = disc_ce_loss(tf.ones_like(real_output), real_output)\n",
    "\n",
    "    generated_loss = disc_ce_loss(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "    total_loss = real_loss + generated_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(rgb_paths, ndvi_paths, image_shape=(256,256,3)):\n",
    "    # Load and resize images\n",
    "    rgb_images = [k.preprocessing.image.load_img(img, target_size=image_shape) for img in rgb_paths]\n",
    "    ndvi_images = [k.preprocessing.image.load_img(img, target_size=image_shape) for img in ndvi_paths]\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    rgb_images = np.array([k.preprocessing.image.img_to_array(img) / 127.5 - 1 for img in rgb_images])\n",
    "    ndvi_images = np.array([k.preprocessing.image.img_to_array(img) / 127.5 - 1 for img in ndvi_images])\n",
    "    \n",
    "    return rgb_images, ndvi_images\n",
    "\n",
    "def get_dataset(rgb_dir, ndvi_dir, image_shape=(256,256,3)):\n",
    "    # Load images\n",
    "    rgb_images = sorted(glob.glob(os.path.join(rgb_dir, '*.jpg')))\n",
    "    ndvi_images = sorted(glob.glob(os.path.join(ndvi_dir, '*.jpg')))\n",
    "    # Check if the number of images is the same\n",
    "    if len(rgb_images) != len(ndvi_images):\n",
    "        raise ValueError(\"Number of RGB and NDVI images do not match.\")\n",
    "    \n",
    "    rgb_images = np.array(rgb_images)\n",
    "    ndvi_images = np.array(ndvi_images)\n",
    "\n",
    "    return rgb_images, ndvi_images\n",
    "    \n",
    "\n",
    "def real_pairs(dataset, n_samples, patch_shape):\n",
    "    # Unpack\n",
    "    trainA, trainB = dataset\n",
    "\n",
    "    # Pick random\n",
    "    ix = np.random.randint(0,trainA.shape[0], n_samples)\n",
    "    X1, X2 = trainA[ix], trainB[ix]\n",
    "\n",
    "    X1, X2 = load_images(X1, X2)\n",
    "    # Label 1 (Real)\n",
    "    y = tf.ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [X1, X2], y\n",
    "\n",
    "def fake_pairs(g_model, samples, patch_shape):\n",
    "    # Generate fake\n",
    "    X = g_model.predict(samples, batch_size=32)\n",
    "    # Label 0 (Fake)\n",
    "    y = tf.zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(step, g_model, d_model, dataset, n_samples=3):\n",
    "    # Select an input sample\n",
    "    [X_realA, X_realB], _ = real_pairs(dataset, n_samples, 1)\n",
    "\n",
    "    # Generate a fake input sample\n",
    "    X_fakeB, _ = fake_pairs(g_model, X_realA, 1)\n",
    "\n",
    "    # Scale pixel values\n",
    "    X_realA = (X_realA + 1) / 2.0\n",
    "    X_fakeB = (X_fakeB + 1) / 2.0\n",
    "    X_realB = (X_realB + 1) / 2.0\n",
    "\n",
    "    for i in range(len(X_fakeB)):\n",
    "        img = X_fakeB[i]\n",
    "        print(img.shape)\n",
    "        fig, axes = plt.subplots(nrows=1,ncols=1, figsize = (3,3))\n",
    "        for col in range(1):\n",
    "            axes.imshow(img)\n",
    "            axes.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filename0 = f'generated_{step + i + 1:06d}.png'\n",
    "        plt.savefig(filename0, bbox_inches='tight', pad_inches=0)        \n",
    "        plt.close()\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=n_samples, figsize=(n_samples * 3, 9))\n",
    "\n",
    "    row_labels = ['Input', 'Generated', 'Ground Truth']\n",
    "\n",
    "    for row, images in enumerate([X_realA, X_fakeB, X_realB]):\n",
    "        for col in range(n_samples):\n",
    "            axes[row, col].imshow(images[col])\n",
    "            axes[row, col].axis('off')\n",
    "\n",
    "        # Add row labels (left side)\n",
    "        axes[row, 0].text(-50, images[0].shape[0] // 2, row_labels[row], fontsize=12,\n",
    "                          va='center', ha='right', color='white', bbox=dict(facecolor='black', alpha=0.7))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    filename1 = f'plot_{step + 1:06d}.png'\n",
    "    plt.savefig(filename1)\n",
    "    plt.close()\n",
    "\n",
    "    # Save models\n",
    "    filename2 = f'generator_model_{step + 1:06d}.keras'\n",
    "    g_model.save(filename2)\n",
    "\n",
    "    filename3 = f'discriminator_model_{step + 1:06d}.keras'\n",
    "    d_model.save(filename3)\n",
    "\n",
    "    print(f'>Saved: {filename1}, {filename2}, {filename3}')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Slightly refactored version of performance() meant to allow you to more easily generate and save outputs for validation\n",
    "\"\"\"\n",
    "def validation(g_model, dataset, n_samples=1):\n",
    "    # Select an input sample\n",
    "    trainA, trainB = dataset\n",
    "    print(trainA)\n",
    "    realA = []\n",
    "    realB = []\n",
    "    fakeB = []\n",
    "\n",
    "    for i in range(len(trainA)):\n",
    "\n",
    "        images = load_images([trainA[i]],[trainB[i]])\n",
    "        realA.append((images[0]+1)/2)\n",
    "        realB.append((images[1]+1)/2)\n",
    "        fakeB.append((g_model.predict(images[0], batch_size=32)+1)/2)\n",
    "\n",
    "    for j in range(len(realA)):\n",
    "        X_fakeB = fakeB[j]\n",
    "        X_realA = realA[j]\n",
    "        X_realB = realB[j]\n",
    "        for i in range(len(X_fakeB)):\n",
    "            img = X_fakeB[i]\n",
    "            print(img.shape)\n",
    "            fig, axes = plt.subplots(nrows=1,ncols=1, figsize = (3,3))\n",
    "            for col in range(1):\n",
    "                axes.imshow(img)\n",
    "                axes.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            filename0 = f'zgenerated_{j + 1:06d}.png'\n",
    "            plt.savefig(filename0, bbox_inches='tight', pad_inches=0)        \n",
    "            plt.close()\n",
    "        print(f'>Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(images, noise_factor=0.05):\n",
    "    noise = noise_factor * tf.random.normal(shape=images.shape)\n",
    "    return images + noise\n",
    "\n",
    "\"\"\"\n",
    "Proceedure for each batch/image training step\n",
    "\"\"\"\n",
    "def train_step(input_image, target, gen_output, step, g_model, d_model):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = g_model(input_image, training = True)\n",
    "\n",
    "        im = tf.convert_to_tensor(input_image)\n",
    "        disc_real_output = d_model([input_image, target], training=True)\n",
    "        disc_fake_output = d_model([im, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_fake_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "        real_loss = disc_ce_loss(tf.ones_like(disc_real_output), disc_real_output)\n",
    "        generated_loss = disc_ce_loss(tf.zeros_like(disc_fake_output), disc_fake_output)\n",
    "    print(f\">{step}, d[Total:{disc_loss:.2f} Real: {real_loss:.2f} Fake:{generated_loss:.2f} ] \\tg[Total: {gen_total_loss:.2f} Gan: {gen_gan_loss:.2f} L1:{gen_l1_loss:.2f}]\")\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          g_model.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               d_model.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          g_model.trainable_variables))\n",
    "    \n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              d_model.trainable_variables))\n",
    "    \n",
    "\"\"\"\n",
    "Main training loop\n",
    "\"\"\"\n",
    "def fit(gen, disc, dataset, steps):\n",
    "    n_patch = disc.output_shape[1]\n",
    "    start = time.time()\n",
    "\n",
    "    # Unpack data\n",
    "    trainA, trainB = dataset\n",
    "    for step in range(steps):\n",
    "        [X_realA, X_realB], _ = real_pairs([trainA, trainB], 1, n_patch)\n",
    "        X_fakeB, _ = fake_pairs(gen, X_realA, n_patch)\n",
    "\n",
    "        X_realA = add_noise(X_realA, 0.02)\n",
    "        X_realB = add_noise(X_realB, 0.02)\n",
    "        X_fakeB = add_noise(X_fakeB, 0.02)\n",
    "        \n",
    "        if (step) % 1000 == 0:\n",
    "            display.clear_output(wait=True)                    \n",
    "            if step != 0:\n",
    "                print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
    "                print(f\"Step: {step//1000}k\")\n",
    "                start = time.time()\n",
    "        if (step) % 1000 == 0:\n",
    "            performance(step, gen, disc, [trainA, trainB])\n",
    "        if (step) % 10_000 == 0:\n",
    "            gen.save(\"generator_model.keras\")\n",
    "            disc.save(\"discriminator_model.keras\") \n",
    "        train_step(X_realA, X_realB,X_fakeB, step, gen, disc)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load previously saved models and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset('dataset/train/inputs/RGB', 'dataset/train/inputs/NDVI', image_shape=(256,256,3))\n",
    "generator = k.models.load_model('generator_model.keras')\n",
    "discriminator = k.models.load_model('discriminator_model.keras')\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(generator, discriminator, dataset, 100_000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as k\n",
    "from keras import layers as l\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(layer_in, n_filters, batchnorm=True):\n",
    "    # Initialize weights\n",
    "    init = k.initializers.RandomNormal(0., 0.02)    # Add the downsampling\n",
    "    d = l.Conv2D(n_filters, (4,4,), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    if(batchnorm):\n",
    "        d = l.BatchNormalization()(d, training=True)\n",
    "    d = l.LeakyReLU(alpha=0.2)(d)\n",
    "    return d\n",
    "\n",
    "def decoder(layer_in, skip_in, n_filters, dropout=True):\n",
    "    # Initialize weights\n",
    "    init = k.initializers.RandomNormal(0., 0.02)    # Add the upsampling\n",
    "    u = l.Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    u = l.BatchNormalization()(u, training=True)\n",
    "    if(dropout):\n",
    "        u = l.Dropout(0.5)(u, training=True)\n",
    "    # Merge\n",
    "    u = l.Concatenate()([u, skip_in])\n",
    "    u = l.Activation('relu')(u)\n",
    "    return u\n",
    "\n",
    "def Generator(image_shape=(256,256,3)):\n",
    "    # Initialize weights\n",
    "    init = k.initializers.RandomNormal(0., 0.02)\n",
    "    # Input\n",
    "    in_image = l.Input(shape=image_shape)\n",
    "\n",
    "    # Encode\n",
    "    e1 = encoder(in_image, 64, False)\n",
    "    e2 = encoder(e1, 128)\n",
    "    e3 = encoder(e2, 256)\n",
    "    e4 = encoder(e3, 512)\n",
    "    e5 = encoder(e4, 512)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = l.Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e5)\n",
    "    b = l.Activation('relu')(b)\n",
    "\n",
    "    # Decode\n",
    "    d1 = decoder(b, e5, 512)\n",
    "    d2 = decoder(d1, e4, 512)\n",
    "    d3 = decoder(d2, e3, 256, dropout=False)\n",
    "    d4 = decoder(d3, e2, 128, dropout=False)\n",
    "    d5 = decoder(d4, e1, 64, dropout=False)\n",
    "\n",
    "    # Output\n",
    "    g = l.Conv2DTranspose(3, (4,4), strides=(2,2), padding = 'same', kernel_initializer=init)(d5)\n",
    "    out_image = l.Activation('tanh')(g)\n",
    "\n",
    "    # Define model\n",
    "    model = k.Model(inputs=in_image, outputs = out_image)\n",
    "    return model\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "LAMBDA = 100\n",
    "\n",
    "def generator_loss(disc_fake_output, generator_output, target_output):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_fake_output), disc_fake_output)\n",
    "\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target_output - generator_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(image_shape):\n",
    "    init = k.initializers.RandomNormal(0., 0.02)\n",
    "    input_image = l.Input(shape=image_shape)\n",
    "    target_image = l.Input(shape=image_shape)\n",
    "    d = l.Concatenate()([input_image, target_image])\n",
    "    \n",
    "    d = l.SpectralNormalization(l.Conv2D(64, (4, 4), strides=2, padding='same', kernel_initializer=init))(d)\n",
    "    d = l.LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = l.SpectralNormalization(l.Conv2D(128, (4, 4), strides=2, padding='same', kernel_initializer=init))(d)\n",
    "    d = l.LayerNormalization()(d, training=True)\n",
    "    d = l.LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = l.SpectralNormalization(l.Conv2D(256, (4, 4), strides=2, padding='same', kernel_initializer=init))(d)\n",
    "    d = l.LayerNormalization()(d, training=True)\n",
    "    d = l.LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    zero_pad1 = l.ZeroPadding2D()(d)\n",
    "    d = l.Conv2D(512, 4, strides=1, kernel_initializer = init,\n",
    "                 use_bias= False)(zero_pad1)\n",
    "    \n",
    "    batchnorm1 = l.BatchNormalization()(d)\n",
    "    d = l.LeakyReLU(alpha=0.2)(batchnorm1)\n",
    "\n",
    "    zero_pad2 = l.ZeroPadding2D()(d)\n",
    "    \n",
    "    d = l.SpectralNormalization(l.Conv2D(1, 4, strides=1, kernel_initializer=init))(zero_pad2)\n",
    "    \n",
    "    model = k.Model([input_image, target_image], d)\n",
    "    return model\n",
    "\n",
    "disc_ce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = disc_ce_loss(tf.ones_like(real_output), real_output)\n",
    "\n",
    "    generated_loss = disc_ce_loss(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "    total_loss = real_loss + generated_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gan(g_model, d_model, image_shape):\n",
    "#     for layer in d_model.layers:\n",
    "#         if not isinstance(layer, l.BatchNormalization):\n",
    "#             layer.trainable = False\n",
    "#     in_src = l.Input(shape=image_shape)\n",
    "#     gen_out = g_model(in_src)\n",
    "#     dis_out = d_model([in_src, gen_out])\n",
    "\n",
    "#     model = k.Model(in_src, [dis_out, gen_out])\n",
    "    \n",
    "#     opt = k.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "#     model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(rgb_paths, ndvi_paths, image_shape=(256,256,3)):\n",
    "    # Load and resize images\n",
    "    rgb_images = [k.preprocessing.image.load_img(img, target_size=image_shape) for img in rgb_paths]\n",
    "    ndvi_images = [k.preprocessing.image.load_img(img, target_size=image_shape) for img in ndvi_paths]\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    rgb_images = np.array([k.preprocessing.image.img_to_array(img) / 127.5 - 1 for img in rgb_images])\n",
    "    ndvi_images = np.array([k.preprocessing.image.img_to_array(img) / 127.5 - 1 for img in ndvi_images])\n",
    "    \n",
    "    return rgb_images, ndvi_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(rgb_paths, ndvi_paths, image_shape=(256,256,3)):\n",
    "    # Load and resize images\n",
    "    rgb_images = [k.preprocessing.image.load_img(img, target_size=image_shape) for img in rgb_paths]\n",
    "    ndvi_images = [k.preprocessing.image.load_img(img, target_size=image_shape) for img in ndvi_paths]\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    rgb_images = np.array([k.preprocessing.image.img_to_array(img) / 127.5 - 1 for img in rgb_images])\n",
    "    ndvi_images = np.array([k.preprocessing.image.img_to_array(img) / 127.5 - 1 for img in ndvi_images])\n",
    "    \n",
    "    return rgb_images, ndvi_images\n",
    "\n",
    "def get_dataset(rgb_dir, ndvi_dir, image_shape=(256,256,3)):\n",
    "    # Load images\n",
    "    rgb_images = sorted(glob.glob(os.path.join(rgb_dir, '*.jpg')))\n",
    "    ndvi_images = sorted(glob.glob(os.path.join(ndvi_dir, '*.jpg')))\n",
    "    # Check if the number of images is the same\n",
    "    if len(rgb_images) != len(ndvi_images):\n",
    "        raise ValueError(\"Number of RGB and NDVI images do not match.\")\n",
    "    \n",
    "    rgb_images = np.array(rgb_images)\n",
    "    ndvi_images = np.array(ndvi_images)\n",
    "\n",
    "    return rgb_images, ndvi_images\n",
    "    \n",
    "\n",
    "def real_pairs(dataset, n_samples, patch_shape):\n",
    "    # Unpack\n",
    "    trainA, trainB = dataset\n",
    "\n",
    "    # Pick random\n",
    "    ix = np.random.randint(0,trainA.shape[0], n_samples)\n",
    "    X1, X2 = trainA[ix], trainB[ix]\n",
    "\n",
    "    X1, X2 = load_images(X1, X2)\n",
    "    # Label 1 (Real)\n",
    "    y = tf.ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [X1, X2], y\n",
    "\n",
    "def fake_pairs(g_model, samples, patch_shape):\n",
    "    # Generate fake\n",
    "    X = g_model.predict(samples, batch_size=32)\n",
    "    # Label 0 (Fake)\n",
    "    y = tf.zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(step, g_model, d_model, dataset, n_samples = 3):\n",
    "    # Select an input sample\n",
    "    [X_realA, X_realB], _ = real_pairs(dataset, n_samples, 1)\n",
    "\n",
    "    # Generate a fake input sample\n",
    "    X_fakeB, _ = fake_pairs(g_model, X_realA, 1)\n",
    "\n",
    "    # Scale pixel values\n",
    "    X_realA = (X_realA + 1) / 2.0\n",
    "    X_fakeB = (X_fakeB + 1) / 2.0\n",
    "    X_realB = (X_realB + 1) / 2.0\n",
    "\n",
    "    # Plot real images\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_realA[i])\n",
    "    \n",
    "    # Plot fake image\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_fakeB[i])\n",
    "\n",
    "    # Plot real image\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 1 + n_samples * 2 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_realB[i])\n",
    "    \n",
    "    filename1 = 'plot_%06d.png' % (step + 1)\n",
    "    plt.savefig(filename1)\n",
    "    plt.close()\n",
    "    filename2 = 'generator_model_%06d.keras' % (step + 1)\n",
    "    g_model.save(filename2)\n",
    "    filename3 = 'discriminator_model_%06d.keras' % (step + 1)\n",
    "    d_model.save(filename3)\n",
    "    print('>Saved: %s, %s, %s' % (filename1, filename2, filename3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(images, noise_factor=0.05):\n",
    "    noise = noise_factor * tf.random.normal(shape=images.shape)\n",
    "    return images + noise\n",
    "\n",
    "\n",
    "import time\n",
    "from IPython import display\n",
    "def train_step(input_image, target, gen_output, step, g_model, d_model):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = g_model(input_image, training = True)\n",
    "\n",
    "        im = tf.convert_to_tensor(input_image)\n",
    "        disc_real_output = d_model([input_image, target], training=True)\n",
    "        disc_fake_output = d_model([im, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_fake_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "        real_loss = disc_ce_loss(tf.ones_like(disc_real_output), disc_real_output)\n",
    "        generated_loss = disc_ce_loss(tf.zeros_like(disc_fake_output), disc_fake_output)\n",
    "    print(f\">{step}, d[Total:{disc_loss:.2f} Real: {real_loss:.2f} Fake:{generated_loss:.2f} ] \\tg[Total: {gen_total_loss:.2f} Gan: {gen_gan_loss:.2f} L1:{gen_l1_loss:.2f}]\")\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          g_model.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               d_model.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          g_model.trainable_variables))\n",
    "    \n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              d_model.trainable_variables))\n",
    "    \n",
    "\n",
    "def fit(gen, disc, dataset, steps, checkpoint):\n",
    "    n_patch = disc.output_shape[1]\n",
    "    start = time.time()\n",
    "\n",
    "    # Unpack data\n",
    "    trainA, trainB = dataset\n",
    "    for step in range(steps):\n",
    "        [X_realA, X_realB], y_real = real_pairs([trainA, trainB], 1, n_patch)\n",
    "        X_fakeB, y_fake = fake_pairs(gen, X_realA, n_patch)\n",
    "\n",
    "        X_realA = add_noise(X_realA, 0.02)\n",
    "        X_realB = add_noise(X_realB, 0.02)\n",
    "        X_fakeB = add_noise(X_fakeB, 0.02)\n",
    "        \n",
    "        if (step) % 1000 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            if step != 0:\n",
    "                print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
    "                print(f\"Step: {step//1000}k\")\n",
    "                start = time.time()\n",
    "                \n",
    "        if (step) % 1000 == 0:\n",
    "            performance(step, gen, disc, [trainA, trainB])\n",
    "        \n",
    "        if (step) % 10_000 == 0:\n",
    "            gen.save(\"generator_model.keras\")\n",
    "            disc.save(\"discriminator_model.keras\")\n",
    "            \n",
    "        train_step(X_realA, X_realB,X_fakeB, step, gen, disc)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "dataset = get_dataset('dataset/train/inputs/RGB', 'dataset/train/inputs/NDVI', image_shape=(256,256,3))\n",
    "generator = k.models.load_model('generator_model.keras')\n",
    "discriminator = k.models.load_model('discriminator_model.keras')\n",
    "# generator.compile(optimizer=generator_optimizer, loss=lambda y_true, y_pred: tf.reduce_mean(y_pred) )\n",
    "# discriminator = Discriminator((256, 256, 3))\n",
    "# discriminator.compile(optimizer=discriminator_optimizer, loss=lambda y_true, y_pred: tf.reduce_mean(y_pred))\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      ">Saved: plot_000001.png, generator_model_000001.keras, discriminator_model_000001.keras\n",
      ">0, d[Total:1.59 Real: 0.92 Fake:0.67 ] \tg[Total: 6.85 Gan: 0.74 L1:0.06]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      ">1, d[Total:0.87 Real: 0.41 Fake:0.46 ] \tg[Total: 47.69 Gan: 1.03 L1:0.47]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      ">2, d[Total:0.58 Real: 0.29 Fake:0.29 ] \tg[Total: 11.94 Gan: 1.40 L1:0.11]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      ">3, d[Total:0.50 Real: 0.20 Fake:0.31 ] \tg[Total: 11.40 Gan: 1.40 L1:0.10]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      ">4, d[Total:1.42 Real: 0.83 Fake:0.59 ] \tg[Total: 8.81 Gan: 0.84 L1:0.08]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      ">5, d[Total:0.96 Real: 0.74 Fake:0.22 ] \tg[Total: 16.93 Gan: 1.69 L1:0.15]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      ">6, d[Total:1.25 Real: 0.84 Fake:0.41 ] \tg[Total: 6.48 Gan: 1.11 L1:0.05]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      ">7, d[Total:1.31 Real: 0.82 Fake:0.49 ] \tg[Total: 4.06 Gan: 0.97 L1:0.03]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      ">8, d[Total:1.35 Real: 0.16 Fake:1.19 ] \tg[Total: 7.55 Gan: 0.45 L1:0.07]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      ">9, d[Total:0.96 Real: 0.29 Fake:0.67 ] \tg[Total: 7.97 Gan: 0.73 L1:0.07]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      ">10, d[Total:1.30 Real: 0.66 Fake:0.65 ] \tg[Total: 5.44 Gan: 0.76 L1:0.05]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      ">11, d[Total:1.50 Real: 1.10 Fake:0.40 ] \tg[Total: 7.86 Gan: 1.20 L1:0.07]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      ">12, d[Total:0.76 Real: 0.31 Fake:0.45 ] \tg[Total: 11.49 Gan: 1.04 L1:0.10]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      ">13, d[Total:0.40 Real: 0.19 Fake:0.22 ] \tg[Total: 11.42 Gan: 1.70 L1:0.10]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      ">14, d[Total:0.61 Real: 0.07 Fake:0.54 ] \tg[Total: 16.23 Gan: 1.02 L1:0.15]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      ">15, d[Total:1.81 Real: 0.48 Fake:1.34 ] \tg[Total: 16.99 Gan: 0.32 L1:0.17]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      ">16, d[Total:1.36 Real: 0.74 Fake:0.61 ] \tg[Total: 3.90 Gan: 0.81 L1:0.03]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      ">17, d[Total:1.25 Real: 0.94 Fake:0.32 ] \tg[Total: 12.01 Gan: 1.38 L1:0.11]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      ">18, d[Total:1.03 Real: 0.25 Fake:0.78 ] \tg[Total: 11.23 Gan: 0.71 L1:0.11]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      ">19, d[Total:1.33 Real: 1.00 Fake:0.33 ] \tg[Total: 12.29 Gan: 1.33 L1:0.11]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      ">20, d[Total:1.38 Real: 0.81 Fake:0.58 ] \tg[Total: 10.23 Gan: 0.83 L1:0.09]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_16156\\3755837754.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m fit(generator, discriminator, dataset, \u001b[32m10000\u001b[39m, checkpoint)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_16156\\306659750.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(gen, disc, dataset, steps, checkpoint)\u001b[39m\n\u001b[32m     58\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (step) % \u001b[32m10_000\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     59\u001b[39m             gen.save(\u001b[33m\"generator_model.keras\"\u001b[39m)\n\u001b[32m     60\u001b[39m             disc.save(\u001b[33m\"discriminator_model.keras\"\u001b[39m)\n\u001b[32m     61\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         train_step(X_realA, X_realB,X_fakeB, step, gen, disc)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_16156\\306659750.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input_image, target, gen_output, step, g_model, d_model)\u001b[39m\n\u001b[32m     22\u001b[39m                                           g_model.trainable_variables)\n\u001b[32m     23\u001b[39m     discriminator_gradients = disc_tape.gradient(disc_loss,\n\u001b[32m     24\u001b[39m                                                d_model.trainable_variables)\n\u001b[32m     25\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     generator_optimizer.apply_gradients(zip(generator_gradients,\n\u001b[32m     27\u001b[39m                                           g_model.trainable_variables))\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m     discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m apply_gradients(self, grads_and_vars):\n\u001b[32m    382\u001b[39m         grads, trainable_variables = zip(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         self.apply(grads, trainable_variables)\n\u001b[32m    384\u001b[39m         \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    385\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._iterations\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    444\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    445\u001b[39m                 grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;28;01min\u001b[39;00m grads]\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m             \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m             self._backend_apply_gradients(grads, trainable_variables)\n\u001b[32m    449\u001b[39m             \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    450\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;28;01min\u001b[39;00m trainable_variables:\n\u001b[32m    451\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m variable.constraint \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    507\u001b[39m             grads = self._clip_gradients(grads)\n\u001b[32m    508\u001b[39m             self._apply_weight_decay(trainable_variables)\n\u001b[32m    509\u001b[39m \n\u001b[32m    510\u001b[39m             \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m             self._backend_update_step(\n\u001b[32m    512\u001b[39m                 grads, trainable_variables, self.learning_rate\n\u001b[32m    513\u001b[39m             )\n\u001b[32m    514\u001b[39m \n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables, learning_rate)\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m trainable_variables\n\u001b[32m    117\u001b[39m         ]\n\u001b[32m    118\u001b[39m         grads_and_vars = list(zip(grads, trainable_variables))\n\u001b[32m    119\u001b[39m         grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[32m    121\u001b[39m             self._distributed_tf_update_step,\n\u001b[32m    122\u001b[39m             self._distribution_strategy,\n\u001b[32m    123\u001b[39m             grads_and_vars,\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fn, strategy, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m   Returns:\n\u001b[32m     48\u001b[39m     The \u001b[38;5;28;01mreturn\u001b[39;00m value of the `fn` call.\n\u001b[32m     49\u001b[39m   \"\"\"\n\u001b[32m     50\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, *args, **kwargs)\n\u001b[32m     52\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     return distribute_lib.get_replica_context().merge_call(\n\u001b[32m     54\u001b[39m         fn, args=args, kwargs=kwargs)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, distribution, grads_and_vars, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n\u001b[32m    132\u001b[39m \n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;28;01min\u001b[39;00m grads_and_vars:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m             distribution.extended.update(\n\u001b[32m    135\u001b[39m                 var,\n\u001b[32m    136\u001b[39m                 apply_grad_to_update_var,\n\u001b[32m    137\u001b[39m                 args=(grad, learning_rate),\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3001\u001b[39m         _get_default_replica_context()):\n\u001b[32m   3002\u001b[39m       fn = autograph.tf_convert(\n\u001b[32m   3003\u001b[39m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[32m   3004\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m self._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._update(var, fn, args, kwargs, group)\n\u001b[32m   3006\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3007\u001b[39m       return self._replica_ctx_update(\n\u001b[32m   3008\u001b[39m           var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   4072\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update(self, var, fn, args, kwargs, group):\n\u001b[32m   4073\u001b[39m     \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4075\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[39m\n\u001b[32m   4077\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):\n\u001b[32m   4078\u001b[39m     \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[32m   4079\u001b[39m     \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[32m   4080\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[32m-> \u001b[39m\u001b[32m4081\u001b[39m       result = fn(*args, **kwargs)\n\u001b[32m   4082\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[32m   4083\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m   4084\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(var, grad, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, gradient, variable, learning_rate)\u001b[39m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m         alpha = lr * ops.sqrt(\u001b[32m1\u001b[39m - beta_2_power) / (\u001b[32m1\u001b[39m - beta_1_power)\n\u001b[32m    131\u001b[39m \n\u001b[32m    132\u001b[39m         self.assign_add(\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m             m, ops.multiply(ops.subtract(gradient, m), \u001b[32m1\u001b[39m - self.beta_1)\n\u001b[32m    134\u001b[39m         )\n\u001b[32m    135\u001b[39m         self.assign_add(\n\u001b[32m    136\u001b[39m             v,\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m   6108\u001b[39m         Output tensor, element-wise product of `x1` \u001b[38;5;28;01mand\u001b[39;00m `x2`.\n\u001b[32m   6109\u001b[39m     \"\"\"\n\u001b[32m   6110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[32m   6111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Multiply().symbolic_call(x1, x2)\n\u001b[32m-> \u001b[39m\u001b[32m6112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.numpy.multiply(x1, x2)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\sparse.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    623\u001b[39m                     x2.indices,\n\u001b[32m    624\u001b[39m                     x2.dense_shape,\n\u001b[32m    625\u001b[39m                 )\n\u001b[32m    626\u001b[39m         \u001b[38;5;66;03m# Default case, no SparseTensor and no IndexedSlices.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(x1, x2)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    588\u001b[39m         getattr(x2, \u001b[33m\"dtype\"\u001b[39m, type(x2)),\n\u001b[32m    589\u001b[39m     )\n\u001b[32m    590\u001b[39m     x1 = convert_to_tensor(x1, dtype)\n\u001b[32m    591\u001b[39m     x2 = convert_to_tensor(x2, dtype)\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.multiply(x1, x2)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m    143\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m    145\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m    522\u001b[39m \n\u001b[32m    523\u001b[39m    * InvalidArgumentError: When `x` \u001b[38;5;28;01mand\u001b[39;00m `y` have incompatible shapes \u001b[38;5;28;01mor\u001b[39;00m types.\n\u001b[32m    524\u001b[39m   \"\"\"\n\u001b[32m    525\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops.mul(x, y, name)\n",
      "\u001b[32mc:\\Users\\UwU\\Documents\\cs-6521\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m   6825\u001b[39m         _ctx, \"Mul\", name, x, y)\n\u001b[32m   6826\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   6827\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   6828\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m6829\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   6830\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   6831\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6832\u001b[39m       return mul_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "fit(generator, discriminator, dataset, 10000, checkpoint)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
